{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analyzing IMDB Data in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Background of the Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This lab uses a dataset of 25,000 IMDB reviews. Each review, comes with a label. A label of 0 is given to a negative review, and a label of 1 is given to a positive review. The goal of this lab is to create a model that will predict the sentiment of a review, based on the words on it. You can see more information about this dataset in the Keras website.\n",
    "\n",
    "Now, the input already comes preprocessed for us for convenience. Each review is encoded as a sequence of indexes, corresponding to the words in the review. The words are ordered by frequency, so the integer 1 corresponds to the most frequent word (\"the\"), the integer 2 to the second most frequent word, etc. By convention, the integer 0 corresponds to unknown words.\n",
    "\n",
    "Then, the sentence is turned into a vector by simply concatenating these integers. For instance, if the sentence is \"To be or not to be.\" and the indices of the words are as follows:\n",
    "* \"to\": 5\n",
    "* \"be\": 8\n",
    "* \"or\": 21\n",
    "* \"not\": 3\n",
    "\n",
    "Then the sentence gets encoded as the vector [5,8,21,3,5,8]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Loading the data\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (it's preloaded in Keras)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000, skip_top=4,\n",
    "                                                     maxlen=None,\n",
    "                                                     seed=113,\n",
    "                                                     start_char=1,\n",
    "                                                     oov_char=2,\n",
    "                                                     index_from=3)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(x_train[0])\n",
    "#print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. One-hot encoding the output\n",
    "Here, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "#print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Building the  model architecture\n",
    "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 644,354\n",
      "Trainable params: 644,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, kernel_initializer='random_uniform', activation='relu', input_dim=1000))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compile the model using a loss function and an optimizer.\n",
    "rms = RMSprop(lr=0.01, rho=0.6, epsilon=0.1, decay=0.0)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer= rms, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      " - 33s - loss: 0.4615 - acc: 0.7798 - val_loss: 0.3636 - val_acc: 0.8408\n",
      "Epoch 2/20\n",
      " - 30s - loss: 0.3526 - acc: 0.8489 - val_loss: 0.3661 - val_acc: 0.8316\n",
      "Epoch 3/20\n",
      " - 30s - loss: 0.3229 - acc: 0.8617 - val_loss: 0.3375 - val_acc: 0.8469\n",
      "Epoch 4/20\n",
      " - 30s - loss: 0.3000 - acc: 0.8728 - val_loss: 0.3491 - val_acc: 0.8426\n",
      "Epoch 5/20\n",
      " - 30s - loss: 0.2801 - acc: 0.8827 - val_loss: 0.3301 - val_acc: 0.8592\n",
      "Epoch 6/20\n",
      " - 14s - loss: 0.2543 - acc: 0.8965 - val_loss: 0.3443 - val_acc: 0.8521\n",
      "Epoch 7/20\n",
      " - 12s - loss: 0.2297 - acc: 0.9069 - val_loss: 0.4107 - val_acc: 0.8330\n",
      "Epoch 8/20\n",
      " - 12s - loss: 0.2027 - acc: 0.9204 - val_loss: 0.4136 - val_acc: 0.8336\n",
      "Epoch 9/20\n",
      " - 12s - loss: 0.1797 - acc: 0.9290 - val_loss: 0.3957 - val_acc: 0.8516\n",
      "Epoch 10/20\n",
      " - 12s - loss: 0.1542 - acc: 0.9399 - val_loss: 0.3984 - val_acc: 0.8550\n",
      "Epoch 11/20\n",
      " - 12s - loss: 0.1322 - acc: 0.9478 - val_loss: 0.4381 - val_acc: 0.8487\n",
      "Epoch 12/20\n",
      " - 12s - loss: 0.1144 - acc: 0.9556 - val_loss: 0.5095 - val_acc: 0.8447\n",
      "Epoch 13/20\n",
      " - 12s - loss: 0.1034 - acc: 0.9598 - val_loss: 0.4713 - val_acc: 0.8518\n",
      "Epoch 14/20\n",
      " - 13s - loss: 0.0940 - acc: 0.9638 - val_loss: 0.5171 - val_acc: 0.8517\n",
      "Epoch 15/20\n",
      " - 12s - loss: 0.0868 - acc: 0.9676 - val_loss: 0.4862 - val_acc: 0.8561\n",
      "Epoch 16/20\n",
      " - 12s - loss: 0.0726 - acc: 0.9728 - val_loss: 0.5730 - val_acc: 0.8528\n",
      "Epoch 17/20\n",
      " - 12s - loss: 0.0601 - acc: 0.9780 - val_loss: 0.5707 - val_acc: 0.8479\n",
      "Epoch 18/20\n",
      " - 12s - loss: 0.0626 - acc: 0.9769 - val_loss: 0.5500 - val_acc: 0.8541\n",
      "Epoch 19/20\n",
      " - 12s - loss: 0.0578 - acc: 0.9782 - val_loss: 0.5811 - val_acc: 0.8494\n",
      "Epoch 20/20\n",
      " - 13s - loss: 0.0480 - acc: 0.9827 - val_loss: 0.6274 - val_acc: 0.8530\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "hist = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6. Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training Accuracy:', 0.99948000000000004)\n",
      "('Model Accuracy: ', 0.85296000000000005)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy:\", score[1])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Model Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
